\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}The Expectation Maximization Algorithm}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.a}Mixture Models}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces MLE Estimators for $\mu $ and $\sigma ^2$ will be complete thrown off by the presence of uniform noise. (kinda looks like ChIP data right?)}}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.b}Outline of the EM Algorithm}{2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The EM Algorithm for GMM}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.c}Proof of the EM}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Sense the KL divergence is always non-negative, $\mathcal  {L}$ provides a lower bound on the incomplete data log-likelihood. Thus maximizing that lower bound will always give us a better estimate for the latent variables/parameters.}}{5}}
